p8105_hw6_gl2761
================
Gonghao Liu
12/3/2022

## Problem 1

To obtain a distribution for $\hat{r}^2$, we’ll follow basically the
same procedure we used for regression coefficients: draw bootstrap
samples; the a model to each; extract the value I’m concerned with; and
summarize. Here, we’ll use `modelr::bootstrap` to draw the samples and
`broom::glance` to produce `r.squared` values.

### Loading data

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

    ## Registered S3 method overwritten by 'hoardr':
    ##   method           from
    ##   print.cache_info httr

    ## using cached file: ~/Library/Caches/R/noaa_ghcnd/USW00094728.dly

    ## date created (size, mb): 2022-12-03 15:04:05 (8.428)

    ## file min/max dates: 1869-01-01 / 2022-12-31

``` r
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

<img src="p8105_hw6_gl2761_files/figure-gfm/unnamed-chunk-1-1.png" width="90%" />

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1
may be a cause for the generally skewed shape of the distribution. If we
wanted to construct a confidence interval for $R^2$, we could take the
2.5% and 97.5% quantiles of the estimates across bootstrap samples.
However, because the shape isn’t symmetric, using the mean +/- 1.96
times the standard error probably wouldn’t work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a
similar approach, with a bit more wrangling before we make our plot.

``` r
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

<img src="p8105_hw6_gl2761_files/figure-gfm/unnamed-chunk-2-1.png" width="90%" />

As with $r^2$, this distribution is somewhat skewed and has some
outliers.

The point of this is not to say you should always use the bootstrap –
it’s possible to establish “large sample” distributions for strange
parameters / values / summaries in a lot of cases, and those are great
to have. But it is helpful to know that there’s a way to do inference
even in tough cases.

## Problem 2

### Read in and clean data

``` r
homicide_df =
  read.csv("./data_WP/homicide-data.csv", na = c("", "Unknown")) %>% 
  mutate(
    city_state = str_c(city, ', ', state),
    resolved = case_when(
           disposition =="Closed without arrest" ~ 0,
           disposition =="Open/No arrest" ~ 0,
           disposition =="Closed by arrest" ~ 1
         )) %>% 
  subset(city_state!="Dallas, TX" & city_state!="Phoenix, AZ" & city_state!="Kansas City, MO" & city_state!="Tulsa, AL") %>%
  filter(victim_race == "White" | victim_race == "Black") %>%
  relocate(city_state)
```

### Analysis for Baltimore

``` r
baltimore_df = 
  homicide_df %>% 
  filter(city_state == "Baltimore, MD")

baltimore_fit = 
  baltimore_df %>% 
  glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) 

baltimore_fit %>% 
  broom::tidy(conf.int = T) %>% 
  mutate(OR = exp(estimate),
         CI_lower = exp(exp(conf.low)),
         CI_upper = exp(exp(conf.high)),
         p_val = rstatix::p_format(p.value, digits = 2)) %>% 
  select(term, OR, CI_lower,CI_upper, p_val) %>% 
  knitr::kable(digits = 3, align = "lccc", 
               col.names = c("Term", "Estimated adjusted OR", "CI lower bound", "CI upper bound", "p-value"))
```

| Term             | Estimated adjusted OR | CI lower bound | CI upper bound | p-value  |
|:-----------------|:---------------------:|:--------------:|:--------------:|:---------|
| (Intercept)      |         1.363         |     2.653      |     6.758      | 0.07     |
| victim_age       |         0.993         |     2.683      |     2.718      | 0.043    |
| victim_raceWhite |         2.320         |     5.205      |     26.468     | \<0.0001 |
| victim_sexMale   |         0.426         |     1.383      |     1.746      | \<0.0001 |

For the city of Baltimore, MD, controlling for all other variables, for
homicides cases whose victim is male are significantly less like to be
resolved than those whose victim is female.

### Analysis for all cities

``` r
cities_fit = homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = map(data, ~glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial(link = "logit"))),
    results = map(models, ~broom::tidy(.x, conf.int = T))) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(conf.low),
    CI_upper = exp(conf.high),
    p_val = rstatix::p_format(p.value, digits = 2)
  ) %>% 
  filter(term == "victim_sexMale") %>% 
  select(city_state, OR, CI_lower,CI_upper, p_val) 

cities_fit %>% 
  knitr::kable(digits = 3, align = "llccc", col.names = c("City", "Estimated adjusted OR", "CI lower bound", "CI upper bound", "p-value"))
```

| City               | Estimated adjusted OR | CI lower bound | CI upper bound | p-value  |
|:-------------------|:----------------------|:--------------:|:--------------:|:--------:|
| Albuquerque, NM    | 1.767                 |     0.825      |     3.762      |  0.1393  |
| Atlanta, GA        | 1.000                 |     0.680      |     1.458      | 0.99968  |
| Baltimore, MD      | 0.426                 |     0.324      |     0.558      | \<0.0001 |
| Baton Rouge, LA    | 0.381                 |     0.204      |     0.684      | 0.00165  |
| Birmingham, AL     | 0.870                 |     0.571      |     1.314      | 0.51115  |
| Boston, MA         | 0.667                 |     0.351      |     1.260      | 0.21213  |
| Buffalo, NY        | 0.521                 |     0.288      |     0.936      | 0.02895  |
| Charlotte, NC      | 0.884                 |     0.551      |     1.391      | 0.60041  |
| Chicago, IL        | 0.410                 |     0.336      |     0.501      | \<0.0001 |
| Cincinnati, OH     | 0.400                 |     0.231      |     0.667      | 0.00065  |
| Columbus, OH       | 0.532                 |     0.377      |     0.748      |  0.0003  |
| Denver, CO         | 0.479                 |     0.233      |     0.962      |  0.0411  |
| Detroit, MI        | 0.582                 |     0.462      |     0.734      | \<0.0001 |
| Durham, NC         | 0.812                 |     0.382      |     1.658      | 0.57611  |
| Fort Worth, TX     | 0.669                 |     0.394      |     1.121      | 0.13117  |
| Fresno, CA         | 1.335                 |     0.567      |     3.048      | 0.49638  |
| Houston, TX        | 0.711                 |     0.557      |     0.906      | 0.00593  |
| Indianapolis, IN   | 0.919                 |     0.678      |     1.241      | 0.58189  |
| Jacksonville, FL   | 0.720                 |     0.536      |     0.965      | 0.02832  |
| Las Vegas, NV      | 0.837                 |     0.606      |     1.151      | 0.27761  |
| Long Beach, CA     | 0.410                 |     0.143      |     1.024      | 0.07176  |
| Los Angeles, CA    | 0.662                 |     0.457      |     0.954      | 0.02793  |
| Louisville, KY     | 0.491                 |     0.301      |     0.784      | 0.00337  |
| Memphis, TN        | 0.723                 |     0.526      |     0.984      | 0.04205  |
| Miami, FL          | 0.515                 |     0.304      |     0.873      | 0.01348  |
| Milwaukee, wI      | 0.727                 |     0.495      |     1.054      | 0.09767  |
| Minneapolis, MN    | 0.947                 |     0.476      |     1.881      | 0.87573  |
| Nashville, TN      | 1.034                 |     0.681      |     1.556      | 0.87289  |
| New Orleans, LA    | 0.585                 |     0.422      |     0.812      | 0.00131  |
| New York, NY       | 0.262                 |     0.133      |     0.485      | \<0.0001 |
| Oakland, CA        | 0.563                 |     0.364      |     0.867      | 0.00937  |
| Oklahoma City, OK  | 0.974                 |     0.623      |     1.520      | 0.90794  |
| Omaha, NE          | 0.382                 |     0.199      |     0.711      | 0.00295  |
| Philadelphia, PA   | 0.496                 |     0.376      |     0.650      | \<0.0001 |
| Pittsburgh, PA     | 0.431                 |     0.263      |     0.696      | 0.00067  |
| Richmond, VA       | 1.006                 |     0.483      |     1.994      | 0.98658  |
| San Antonio, TX    | 0.705                 |     0.393      |     1.238      | 0.23034  |
| Sacramento, CA     | 0.669                 |     0.326      |     1.314      | 0.25481  |
| Savannah, GA       | 0.867                 |     0.419      |     1.780      | 0.69735  |
| San Bernardino, CA | 0.500                 |     0.166      |     1.462      | 0.20567  |
| San Diego, CA      | 0.413                 |     0.191      |     0.830      | 0.01722  |
| San Francisco, CA  | 0.608                 |     0.312      |     1.155      | 0.13362  |
| St. Louis, MO      | 0.703                 |     0.530      |     0.932      | 0.01439  |
| Stockton, CA       | 1.352                 |     0.626      |     2.994      | 0.44745  |
| Tampa, FL          | 0.808                 |     0.340      |     1.860      | 0.61939  |
| Tulsa, OK          | 0.976                 |     0.609      |     1.544      | 0.91746  |
| Washington, DC     | 0.691                 |     0.466      |     1.014      | 0.06167  |

``` r
cities_fit %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>%
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(x = "City", y = "Estimated OR with CI")
```

<img src="p8105_hw6_gl2761_files/figure-gfm/unnamed-chunk-6-1.png" width="90%" />

For most of the cities, the estimated odds ratio is less than 1, and
about half of cities have CI that doesn’t contain 1, which means for
most of the cities, controlling for all other variables, for homicides
cases whose victim is male are significantly less like to be resolved
than those whose victim is female.

## Problem 3

### Load and clean the data for regression analysis.

``` r
birthweight_df = read.csv("./birthweight.csv") %>% 
  janitor::clean_names()

# Check missing value
sum(is.na(birthweight_df))
```

    ## [1] 0

``` r
birthweight_df = birthweight_df %>% 
  mutate(
    babysex = factor(babysex, ordered = FALSE),
    frace = factor(frace, ordered = FALSE),
    malform = factor(malform, ordered = FALSE),
    mrace = factor(mrace, ordered = FALSE)
  )
```

### Propose a regression model for birthweight.

``` r
model_1 = lm(bwt ~ ppbmi + delwt + gaweeks + momage + smoken + wtgain, data = birthweight_df) 

model_1 %>% broom::tidy()
```

    ## # A tibble: 7 × 5
    ##   term        estimate std.error statistic   p.value
    ##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>
    ## 1 (Intercept)   -39.6     97.1      -0.408 6.84e-  1
    ## 2 ppbmi         -26.7      4.05     -6.59  5.04e- 11
    ## 3 delwt           7.92     0.641    12.3   1.87e- 34
    ## 4 gaweeks        58.6      2.16     27.1   7.00e-150
    ## 5 momage         12.5      1.76      7.11  1.40e- 12
    ## 6 smoken         -7.78     0.906    -8.59  1.21e- 17
    ## 7 wtgain          1.92     0.912     2.11  3.52e-  2

``` r
plot = birthweight_df %>% 
  add_residuals(model_1) %>% 
  add_predictions(model_1) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 0, intercept = 0, color = "red") +
  labs(
    title = "Residuals vs Fitted value Plot",
    x = "Fitted value",
    y = "Residuals"
  ) +
  theme(plot.title = element_text(hjust = 0.5))

plot
```

<img src="p8105_hw6_gl2761_files/figure-gfm/unnamed-chunk-8-1.png" width="90%" />

Describe modeling process:

1.  Firstly, load and clean data;

2.  Then I think “mother’s weight at delivery (pounds)”, ‘gestational
    age in weeks’, ‘mother’s age at delivery’, ‘average number of
    cigarettes smoked per day during pregnancy’, ‘mother’s weight gain
    during pregnancy (pounds)’ and “mother’s pre-pregnancy BMI” may
    influence baby’s birth weight, so I use these two variables as the
    predictors of the model;

3.  Then I drew a “Residuals vs Fitted value Plot”. From the plot,
    residual values are bounce around 0, which means they are evenly
    distributed around 0. Therefore, it is a reasonable model and I keep
    this model.

### Compare my model to two others:

``` r
model_2 = lm(bwt ~ blength + gaweeks, data = birthweight_df)
model_3 = lm(bwt ~ bhead + blength + babysex + bhead * blength + bhead * babysex + blength * babysex + bhead * blength * babysex, data = birthweight_df)

cv_df = crossv_mc(birthweight_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)) %>% 
  mutate(
    mod_1 = map(.x = train, ~lm(bwt ~ ppbmi + delwt, data = .x)),
    mod_2 = map(.x = train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    mod_3 = map(.x = train, ~lm(bwt ~ bhead + blength + babysex + bhead * blength + bhead * babysex + blength * babysex + bhead * blength * babysex, data = .x))) %>% 
  mutate(
    rmse_1 = map2_dbl(.x = mod_1, .y = test, ~rmse(model = .x, data = .y)),
    rmse_2 = map2_dbl(.x = mod_2, .y = test, ~rmse(model = .x, data = .y)),
    rmse_3 = map2_dbl(.x = mod_3, .y = test, ~rmse(model = .x, data = .y))) %>%
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) 

cv_df %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin() +
  labs(title = "RMSEs for Each Model",
       x = "Model",
       y = "RMSE") +
  theme(plot.title = element_text(hjust = 0.5))
```

<img src="p8105_hw6_gl2761_files/figure-gfm/unnamed-chunk-9-1.png" width="90%" />

Based on the violin plot, we can find that model 1 has a higher error
rate than the other two models and model 3 has the smallest RMSE value.
Model 3 is more accurate than model 1 and model 2.
